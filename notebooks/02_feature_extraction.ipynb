{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7198a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feature Extraction Pipeline\n",
    "File: notebooks/02_feature_extraction.ipynb (or scripts/extract_features.py)\n",
    "\n",
    "This script processes the entire dataset and extracts features from all items.\n",
    "Features are saved for later use in matching algorithms.\n",
    "\"\"\"\n",
    "\n",
    "from feature_extraction.text_features import TextFeatureExtractor\n",
    "from feature_extraction.image_features import ImageFeatureExtractor\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "\n",
    "class FeatureExtractionPipeline:\n",
    "    \"\"\"Pipeline to extract and save features from all items\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path, output_path):\n",
    "        \"\"\"\n",
    "        Initialize pipeline\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Path to dataset directory\n",
    "            output_path: Path to save extracted features\n",
    "        \"\"\"\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Initialize extractors\n",
    "        self.image_extractor = ImageFeatureExtractor(target_size=(224, 224))\n",
    "        self.text_extractor = TextFeatureExtractor(max_features=100)\n",
    "\n",
    "        # Load metadata\n",
    "        self.metadata = None\n",
    "        self.df = None\n",
    "\n",
    "    def load_metadata(self):\n",
    "        \"\"\"Load metadata from JSON file\"\"\"\n",
    "        metadata_path = self.dataset_path / 'metadata_labelled.json'\n",
    "\n",
    "        print(f\"Loading metadata from: {metadata_path}\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "\n",
    "        self.df = pd.DataFrame(self.metadata)\n",
    "        print(f\"âœ“ Loaded {len(self.df)} items\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def fit_text_extractor(self):\n",
    "        \"\"\"Fit TF-IDF vectorizer on all descriptions\"\"\"\n",
    "        print(\"\\nFitting TF-IDF vectorizer...\")\n",
    "\n",
    "        # Get all descriptions (only labelled ones)\n",
    "        descriptions = self.df[self.df['labelled']\n",
    "                               == True]['description'].tolist()\n",
    "\n",
    "        if len(descriptions) == 0:\n",
    "            print(\"âš  No descriptions found to fit TF-IDF\")\n",
    "            return\n",
    "\n",
    "        self.text_extractor.fit_tfidf(descriptions)\n",
    "\n",
    "        # Save fitted vectorizer\n",
    "        vectorizer_path = self.output_path / 'tfidf_vectorizer.pkl'\n",
    "        with open(vectorizer_path, 'wb') as f:\n",
    "            pickle.dump(self.text_extractor.tfidf_vectorizer, f)\n",
    "\n",
    "        print(f\"âœ“ Saved TF-IDF vectorizer to: {vectorizer_path}\")\n",
    "\n",
    "    def extract_features_for_item(self, item_data, include_sift=False):\n",
    "        \"\"\"\n",
    "        Extract features for a single item\n",
    "        \n",
    "        Args:\n",
    "            item_data: Dictionary with item metadata\n",
    "            include_sift: Whether to include SIFT features\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with extracted features\n",
    "        \"\"\"\n",
    "        item_id = item_data['id']\n",
    "        category = item_data['category']\n",
    "        filename = item_data['filename']\n",
    "        description = item_data.get('description', '')\n",
    "\n",
    "        # Image path\n",
    "        image_path = self.dataset_path / 'images' / category / filename\n",
    "\n",
    "        features = {\n",
    "            'id': item_id,\n",
    "            'category': category,\n",
    "            'filename': filename,\n",
    "            'status': item_data['status'],\n",
    "            'timestamp': item_data['timestamp'],\n",
    "            'has_description': item_data['labelled']\n",
    "        }\n",
    "\n",
    "        # Extract image features\n",
    "        try:\n",
    "            img_result = self.image_extractor.extract_all_features(\n",
    "                image_path,\n",
    "                include_sift=include_sift\n",
    "            )\n",
    "            features['image_features'] = img_result['feature_vector']\n",
    "            features['image_feature_dims'] = img_result['feature_dims']\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Error extracting image features for {item_id}: {e}\")\n",
    "            features['image_features'] = None\n",
    "\n",
    "        # Extract text features\n",
    "        try:\n",
    "            if description and item_data['labelled']:\n",
    "                text_result = self.text_extractor.extract_all_features(\n",
    "                    description,\n",
    "                    category=category\n",
    "                )\n",
    "                features['text_features'] = text_result['feature_vector']\n",
    "                features['text_feature_dims'] = text_result['feature_dims']\n",
    "                features['extracted_keywords'] = text_result['extracted_keywords']\n",
    "            else:\n",
    "                # No description - use zeros\n",
    "                features['text_features'] = np.zeros(\n",
    "                    self.text_extractor.get_feature_vector_size())\n",
    "                features['text_feature_dims'] = {}\n",
    "                features['extracted_keywords'] = {}\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Error extracting text features for {item_id}: {e}\")\n",
    "            features['text_features'] = None\n",
    "\n",
    "        return features\n",
    "\n",
    "    def extract_all_features(self, include_sift=False, save_individual=False):\n",
    "        \"\"\"\n",
    "        Extract features for all items in dataset\n",
    "        \n",
    "        Args:\n",
    "            include_sift: Whether to include SIFT features (slower)\n",
    "            save_individual: Save individual feature files per item\n",
    "            \n",
    "        Returns:\n",
    "            List of feature dictionaries\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EXTRACTING FEATURES FROM ALL ITEMS\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        all_features = []\n",
    "        errors = []\n",
    "\n",
    "        # Process each item\n",
    "        for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Extracting features\"):\n",
    "            try:\n",
    "                features = self.extract_features_for_item(\n",
    "                    row.to_dict(),\n",
    "                    include_sift=include_sift\n",
    "                )\n",
    "                all_features.append(features)\n",
    "\n",
    "                # Save individual file if requested\n",
    "                if save_individual:\n",
    "                    item_file = self.output_path / f\"item_{features['id']}.pkl\"\n",
    "                    with open(item_file, 'wb') as f:\n",
    "                        pickle.dump(features, f)\n",
    "\n",
    "            except Exception as e:\n",
    "                errors.append({'id': row['id'], 'error': str(e)})\n",
    "                print(f\"\\nâš  Error processing item {row['id']}: {e}\")\n",
    "\n",
    "        print(\n",
    "            f\"\\nâœ“ Successfully extracted features for {len(all_features)} items\")\n",
    "\n",
    "        if errors:\n",
    "            print(f\"âš  Encountered {len(errors)} errors\")\n",
    "            error_file = self.output_path / 'extraction_errors.json'\n",
    "            with open(error_file, 'w') as f:\n",
    "                json.dump(errors, f, indent=2)\n",
    "            print(f\"  Error log saved to: {error_file}\")\n",
    "\n",
    "        return all_features\n",
    "\n",
    "    def create_feature_matrices(self, all_features):\n",
    "        \"\"\"\n",
    "        Create numpy matrices for efficient computation\n",
    "        \n",
    "        Args:\n",
    "            all_features: List of feature dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with feature matrices and metadata\n",
    "        \"\"\"\n",
    "        print(\"\\nCreating feature matrices...\")\n",
    "\n",
    "        # Separate by status\n",
    "        lost_features = [f for f in all_features if f['status'] == 'lost']\n",
    "        found_features = [f for f in all_features if f['status'] == 'found']\n",
    "\n",
    "        print(f\"  Lost items: {len(lost_features)}\")\n",
    "        print(f\"  Found items: {len(found_features)}\")\n",
    "\n",
    "        # Create matrices\n",
    "        matrices = {\n",
    "            'lost': {\n",
    "                'ids': [f['id'] for f in lost_features],\n",
    "                'categories': [f['category'] for f in lost_features],\n",
    "                'image_features': np.array([f['image_features'] for f in lost_features\n",
    "                                           if f['image_features'] is not None]),\n",
    "                'text_features': np.array([f['text_features'] for f in lost_features\n",
    "                                          if f['text_features'] is not None]),\n",
    "                'timestamps': [f['timestamp'] for f in lost_features],\n",
    "                'has_description': [f['has_description'] for f in lost_features]\n",
    "            },\n",
    "            'found': {\n",
    "                'ids': [f['id'] for f in found_features],\n",
    "                'categories': [f['category'] for f in found_features],\n",
    "                'image_features': np.array([f['image_features'] for f in found_features\n",
    "                                           if f['image_features'] is not None]),\n",
    "                'text_features': np.array([f['text_features'] for f in found_features\n",
    "                                          if f['text_features'] is not None]),\n",
    "                'timestamps': [f['timestamp'] for f in found_features],\n",
    "                'has_description': [f['has_description'] for f in found_features]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add combined features (concatenate image + text)\n",
    "        for status in ['lost', 'found']:\n",
    "            img_features = matrices[status]['image_features']\n",
    "            txt_features = matrices[status]['text_features']\n",
    "\n",
    "            if len(img_features) > 0 and len(txt_features) > 0:\n",
    "                matrices[status]['combined_features'] = np.concatenate(\n",
    "                    [img_features, txt_features], axis=1\n",
    "                )\n",
    "\n",
    "        return matrices\n",
    "\n",
    "    def save_features(self, all_features, matrices):\n",
    "        \"\"\"\n",
    "        Save extracted features to disk\n",
    "        \n",
    "        Args:\n",
    "            all_features: List of all feature dictionaries\n",
    "            matrices: Feature matrices\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SAVING FEATURES\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # Save complete feature list\n",
    "        features_file = self.output_path / 'all_features.pkl'\n",
    "        with open(features_file, 'wb') as f:\n",
    "            pickle.dump(all_features, f)\n",
    "        print(f\"âœ“ Saved all features to: {features_file}\")\n",
    "\n",
    "        # Save feature matrices\n",
    "        matrices_file = self.output_path / 'feature_matrices.pkl'\n",
    "        with open(matrices_file, 'wb') as f:\n",
    "            pickle.dump(matrices, f)\n",
    "        print(f\"âœ“ Saved feature matrices to: {matrices_file}\")\n",
    "\n",
    "        # Save metadata DataFrame with feature info\n",
    "        feature_df = pd.DataFrame([{\n",
    "            'id': f['id'],\n",
    "            'category': f['category'],\n",
    "            'status': f['status'],\n",
    "            'has_description': f['has_description'],\n",
    "            'has_image_features': f['image_features'] is not None,\n",
    "            'has_text_features': f['text_features'] is not None,\n",
    "            'image_feature_dim': len(f['image_features']) if f['image_features'] is not None else 0,\n",
    "            'text_feature_dim': len(f['text_features']) if f['text_features'] is not None else 0,\n",
    "        } for f in all_features])\n",
    "\n",
    "        df_file = self.output_path / 'feature_summary.csv'\n",
    "        feature_df.to_csv(df_file, index=False)\n",
    "        print(f\"âœ“ Saved feature summary to: {df_file}\")\n",
    "\n",
    "        # Create statistics report\n",
    "        self.create_statistics_report(all_features, matrices)\n",
    "\n",
    "    def create_statistics_report(self, all_features, matrices):\n",
    "        \"\"\"Create a statistics report about extracted features\"\"\"\n",
    "\n",
    "        report = []\n",
    "        report.append(\"=\"*80)\n",
    "        report.append(\"FEATURE EXTRACTION STATISTICS\")\n",
    "        report.append(\"=\"*80)\n",
    "        report.append(\"\")\n",
    "\n",
    "        # Overall statistics\n",
    "        report.append(\"Overall Statistics:\")\n",
    "        report.append(f\"  Total items processed: {len(all_features)}\")\n",
    "        report.append(f\"  Lost items: {len(matrices['lost']['ids'])}\")\n",
    "        report.append(f\"  Found items: {len(matrices['found']['ids'])}\")\n",
    "        report.append(\"\")\n",
    "\n",
    "        # Feature dimensions\n",
    "        sample = all_features[0]\n",
    "        if sample['image_features'] is not None:\n",
    "            report.append(\n",
    "                f\"  Image feature dimension: {len(sample['image_features'])}\")\n",
    "        if sample['text_features'] is not None:\n",
    "            report.append(\n",
    "                f\"  Text feature dimension: {len(sample['text_features'])}\")\n",
    "        report.append(\"\")\n",
    "\n",
    "        # Category breakdown\n",
    "        report.append(\"By Category:\")\n",
    "        for category in sorted(set([f['category'] for f in all_features])):\n",
    "            cat_items = [f for f in all_features if f['category'] == category]\n",
    "            cat_lost = len([f for f in cat_items if f['status'] == 'lost'])\n",
    "            cat_found = len([f for f in cat_items if f['status'] == 'found'])\n",
    "            report.append(\n",
    "                f\"  {category:.<20} Total: {len(cat_items):>3}  (Lost: {cat_lost:>3}, Found: {cat_found:>3})\")\n",
    "        report.append(\"\")\n",
    "\n",
    "        # Description availability\n",
    "        with_desc = len([f for f in all_features if f['has_description']])\n",
    "        report.append(\n",
    "            f\"Items with descriptions: {with_desc} ({with_desc/len(all_features)*100:.1f}%)\")\n",
    "        report.append(\"\")\n",
    "\n",
    "        # Save report\n",
    "        report_text = \"\\n\".join(report)\n",
    "        print(\"\\n\" + report_text)\n",
    "\n",
    "        report_file = self.output_path / 'extraction_report.txt'\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(report_text)\n",
    "        print(f\"\\nâœ“ Saved extraction report to: {report_file}\")\n",
    "\n",
    "    def run(self, include_sift=False):\n",
    "        \"\"\"\n",
    "        Run the complete feature extraction pipeline\n",
    "        \n",
    "        Args:\n",
    "            include_sift: Whether to include SIFT features\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FEATURE EXTRACTION PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Step 1: Load metadata\n",
    "        self.load_metadata()\n",
    "\n",
    "        # Step 2: Fit text extractor\n",
    "        self.fit_text_extractor()\n",
    "\n",
    "        # Step 3: Extract all features\n",
    "        all_features = self.extract_all_features(include_sift=include_sift)\n",
    "\n",
    "        # Step 4: Create matrices\n",
    "        matrices = self.create_feature_matrices(all_features)\n",
    "\n",
    "        # Step 5: Save everything\n",
    "        self.save_features(all_features, matrices)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"âœ“ FEATURE EXTRACTION PIPELINE COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        return all_features, matrices\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure paths\n",
    "    DATASET_PATH = Path('../dataset')\n",
    "    OUTPUT_PATH = Path('../features')\n",
    "\n",
    "    # Run pipeline\n",
    "    pipeline = FeatureExtractionPipeline(DATASET_PATH, OUTPUT_PATH)\n",
    "\n",
    "    # Extract features (set include_sift=True if you want SIFT features)\n",
    "    all_features, matrices = pipeline.run(include_sift=False)\n",
    "\n",
    "    print(\"\\nðŸ“‚ Output files created:\")\n",
    "    print(f\"  â€¢ all_features.pkl - Complete feature data for all items\")\n",
    "    print(f\"  â€¢ feature_matrices.pkl - Numpy matrices for efficient computation\")\n",
    "    print(f\"  â€¢ feature_summary.csv - Summary statistics\")\n",
    "    print(f\"  â€¢ tfidf_vectorizer.pkl - Fitted TF-IDF vectorizer\")\n",
    "    print(f\"  â€¢ extraction_report.txt - Detailed statistics report\")\n",
    "\n",
    "    print(\"\\nðŸŽ¯ Next steps:\")\n",
    "    print(\"  1. Verify feature extraction quality\")\n",
    "    print(\"  2. Implement similarity computation\")\n",
    "    print(\"  3. Build matching algorithm\")\n",
    "    print(\"  4. Evaluate on test set\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
